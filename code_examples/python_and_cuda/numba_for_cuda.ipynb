{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e876a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, cuda\n",
    "import time as time_module\n",
    "\n",
    "# 1. Pure Python version (baseline - slowest)\n",
    "def vector_add_python(a, b, c):\n",
    "    for i in range(len(a)):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# 2. Numba CPU version (JIT compiled)\n",
    "@jit(nopython=True)\n",
    "def vector_add_numba_cpu(a, b, c):\n",
    "    for i in range(len(a)):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# 3. Numba CUDA version (GPU kernel)\n",
    "@cuda.jit\n",
    "def vector_add_cuda_kernel(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "def vector_add_cuda(a, b):\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    \n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    \n",
    "    c = d_c.copy_to_host()\n",
    "    return c\n",
    "\n",
    "def vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid):\n",
    "    \"\"\"GPU kernel without memory transfers - data already on GPU\"\"\"\n",
    "    vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    cuda.synchronize()\n",
    "\n",
    "def multiple_operations_gpu(a, b, n_ops=10):\n",
    "    \"\"\"Perform multiple additions - data stays on GPU\"\"\"\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    \n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    for _ in range(n_ops):\n",
    "        vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    \n",
    "    cuda.synchronize()\n",
    "    return d_c.copy_to_host()\n",
    "\n",
    "def benchmark(func, *args, name=\"Function\", warmup=True):\n",
    "    if warmup:\n",
    "        func(*args)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time_module.perf_counter()\n",
    "        result = func(*args)\n",
    "        end = time_module.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    return avg_time, result\n",
    "\n",
    "def check_cuda():\n",
    "    try:\n",
    "        cuda.select_device(0)\n",
    "        cuda.current_context()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ CUDA not available: {e}\\n\")\n",
    "        return False\n",
    "\n",
    "def format_time(ms):\n",
    "    \"\"\"Format time with appropriate units\"\"\"\n",
    "    if ms >= 1000:\n",
    "        return f\"{ms/1000:.2f} s \"\n",
    "    elif ms >= 1:\n",
    "        return f\"{ms:.2f} ms\"\n",
    "    else:\n",
    "        return f\"{ms*1000:.2f} μs\"\n",
    "\n",
    "def print_header(title, char=\"=\", width=80):\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(f\"{title:^{width}}\")\n",
    "    print(f\"{char * width}\")\n",
    "\n",
    "def print_section(title, width=80):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * width)\n",
    "\n",
    "def print_result(label, time_ms, width=50):\n",
    "    time_str = format_time(time_ms)\n",
    "    print(f\"  {label:<{width-15}} {time_str:>12}\")\n",
    "\n",
    "def print_speedup(label, speedup, width=50):\n",
    "    if speedup >= 1:\n",
    "        print(f\"  {label:<{width-15}} {speedup:>10.1f}x faster\")\n",
    "    else:\n",
    "        print(f\"  {label:<{width-15}} {1/speedup:>10.1f}x slower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75cc99e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           VECTOR ADDITION BENCHMARK                            \n",
      "================================================================================\n",
      "Array size: 10,000,000 elements (40.0 MB per array)\n",
      "\n",
      "1. Pure Python (baseline)\n",
      "--------------------------------------------------------------------------------\n",
      "  Execution time                         695.77 ms\n",
      "\n",
      "2. Numba CPU (JIT compiled)\n",
      "--------------------------------------------------------------------------------\n",
      "  Execution time                           3.06 ms\n",
      "  vs Pure Python                           227.0x faster\n",
      "\n",
      "3. Numba CUDA (with memory transfers)\n",
      "--------------------------------------------------------------------------------\n",
      "  Total time (with transfers)             11.80 ms\n",
      "  vs Pure Python                            59.0x faster\n",
      "  vs Numba CPU                               3.8x slower\n",
      "\n",
      "   GPU Time Breakdown\n",
      "--------------------------------------------------------------------------------\n",
      "  Transfer to GPU                          4.87 ms\n",
      "  Kernel execution                        79.75 μs\n",
      "  Transfer from GPU                        6.01 ms\n",
      "--------------------------------------------------------------------------------\n",
      "  TOTAL                                   10.96 ms\n",
      "\n",
      "  💡 Memory transfer takes 136x longer than computation!\n",
      "\n",
      "4. Multiple Operations (data stays on GPU)\n",
      "--------------------------------------------------------------------------------\n",
      "  CPU (10 operations)                     28.48 ms\n",
      "  GPU (10 operations)                     12.51 ms\n",
      "  GPU vs CPU (10 ops)                        2.3x faster\n",
      "\n",
      "VERIFICATION\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ CPU results match Python:  PASS\n",
      "  ✓ GPU results match Python:  PASS\n",
      "\n",
      "================================================================================\n",
      "                              PERFORMANCE SUMMARY                               \n",
      "================================================================================\n",
      "\n",
      "┌─────────────────────────────────────┬──────────────┬──────────────┐\n",
      "│ Implementation                      │ Time         │ Speedup      │\n",
      "├─────────────────────────────────────┼──────────────┼──────────────┤\n",
      "│ Pure Python (baseline)              │  695.77 ms   │ 1.0x         │\n",
      "│ Numba CPU                           │    3.06 ms   │   227x faster │\n",
      "│ GPU (with transfers)                │   11.80 ms   │    59x faster │\n",
      "│ GPU (kernel only)                   │   79.75 μs   │    38x faster │\n",
      "│ GPU (10 ops, keeps data)            │   12.51 ms   │   2.3x faster │\n",
      "└─────────────────────────────────────┴──────────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "N = 10_000_000\n",
    "a = np.random.rand(N).astype(np.float32)\n",
    "b = np.random.rand(N).astype(np.float32)\n",
    "\n",
    "print_header(\"VECTOR ADDITION BENCHMARK\")\n",
    "print(f\"Array size: {N:,} elements ({N*4/1e6:.1f} MB per array)\")\n",
    "\n",
    "cuda_available = check_cuda()\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# ========================================================================\n",
    "# 1. Pure Python\n",
    "# ========================================================================\n",
    "print_section(\"1. Pure Python (baseline)\")\n",
    "c_python = np.zeros_like(a)\n",
    "time_python, _ = benchmark(vector_add_python, a, b, c_python, \n",
    "                            name=\"Pure Python\", warmup=False)\n",
    "results['python'] = time_python\n",
    "print_result(\"Execution time\", time_python * 1000)\n",
    "\n",
    "# ========================================================================\n",
    "# 2. Numba CPU\n",
    "# ========================================================================\n",
    "print_section(\"2. Numba CPU (JIT compiled)\")\n",
    "c_cpu = np.zeros_like(a)\n",
    "time_cpu, _ = benchmark(vector_add_numba_cpu, a, b, c_cpu, name=\"Numba CPU\")\n",
    "results['cpu'] = time_cpu\n",
    "print_result(\"Execution time\", time_cpu * 1000)\n",
    "print_speedup(\"vs Pure Python\", time_python / time_cpu)\n",
    "\n",
    "# ========================================================================\n",
    "# 3. GPU Benchmarks\n",
    "# ========================================================================\n",
    "if cuda_available:\n",
    "    print_section(\"3. Numba CUDA (with memory transfers)\")\n",
    "    time_cuda, c_cuda = benchmark(vector_add_cuda, a, b, name=\"GPU\")\n",
    "    results['gpu_total'] = time_cuda\n",
    "    print_result(\"Total time (with transfers)\", time_cuda * 1000)\n",
    "    print_speedup(\"vs Pure Python\", time_python / time_cuda)\n",
    "    print_speedup(\"vs Numba CPU\", time_cpu / time_cuda)\n",
    "    \n",
    "    # Break down GPU time\n",
    "    print_section(\"   GPU Time Breakdown\")\n",
    "    \n",
    "    # Transfer TO GPU\n",
    "    start = time_module.perf_counter()\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    transfer_to = time_module.perf_counter() - start\n",
    "    results['transfer_to'] = transfer_to\n",
    "    \n",
    "    # Kernel execution\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time_module.perf_counter()\n",
    "        vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid)\n",
    "        times.append(time_module.perf_counter() - start)\n",
    "    kernel_time = np.mean(times)\n",
    "    results['kernel'] = kernel_time\n",
    "    \n",
    "    # Transfer FROM GPU\n",
    "    start = time_module.perf_counter()\n",
    "    result = d_c.copy_to_host()\n",
    "    transfer_from = time_module.perf_counter() - start\n",
    "    results['transfer_from'] = transfer_from\n",
    "    \n",
    "    total_breakdown = transfer_to + kernel_time + transfer_from\n",
    "    \n",
    "    print_result(\"Transfer to GPU\", transfer_to * 1000)\n",
    "    print_result(\"Kernel execution\", kernel_time * 1000)\n",
    "    print_result(\"Transfer from GPU\", transfer_from * 1000)\n",
    "    print(\"-\" * 80)\n",
    "    print_result(\"TOTAL\", total_breakdown * 1000)\n",
    "    \n",
    "    print(f\"\\n  💡 Memory transfer takes {(transfer_to + transfer_from)/kernel_time:.0f}x longer than computation!\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 4. Multiple Operations\n",
    "    # ====================================================================\n",
    "    print_section(\"4. Multiple Operations (data stays on GPU)\")\n",
    "    n_ops = 10\n",
    "    \n",
    "    # CPU\n",
    "    c_cpu = np.zeros_like(a)\n",
    "    start = time_module.perf_counter()\n",
    "    for _ in range(n_ops):\n",
    "        vector_add_numba_cpu(a, b, c_cpu)\n",
    "    time_cpu_multi = time_module.perf_counter() - start\n",
    "    results['cpu_multi'] = time_cpu_multi\n",
    "    \n",
    "    # GPU\n",
    "    time_gpu_multi, _ = benchmark(multiple_operations_gpu, a, b, n_ops, warmup=True)\n",
    "    results['gpu_multi'] = time_gpu_multi\n",
    "    \n",
    "    print_result(f\"CPU ({n_ops} operations)\", time_cpu_multi * 1000)\n",
    "    print_result(f\"GPU ({n_ops} operations)\", time_gpu_multi * 1000)\n",
    "    print_speedup(f\"GPU vs CPU ({n_ops} ops)\", time_cpu_multi / time_gpu_multi)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Verification\n",
    "    # ====================================================================\n",
    "    print_section(\"VERIFICATION\")\n",
    "    cpu_match = np.allclose(c_python, c_cpu)\n",
    "    gpu_match = np.allclose(c_python, c_cuda)\n",
    "    \n",
    "    print(f\"  ✓ CPU results match Python:  {'PASS' if cpu_match else 'FAIL'}\")\n",
    "    print(f\"  ✓ GPU results match Python:  {'PASS' if gpu_match else 'FAIL'}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Summary Table - Cleaner Format\n",
    "    # ====================================================================\n",
    "    print_header(\"PERFORMANCE SUMMARY\", \"=\")\n",
    "    \n",
    "    print(\"\\n┌─────────────────────────────────────┬──────────────┬──────────────┐\")\n",
    "    print(\"│ Implementation                      │ Time         │ Speedup      │\")\n",
    "    print(\"├─────────────────────────────────────┼──────────────┼──────────────┤\")\n",
    "    print(f\"│ Pure Python (baseline)              │ {format_time(results['python']*1000):>10}   │ 1.0x         │\")\n",
    "    print(f\"│ Numba CPU                           │ {format_time(results['cpu']*1000):>10}   │ {results['python']/results['cpu']:>5.0f}x faster │\")\n",
    "    print(f\"│ GPU (with transfers)                │ {format_time(results['gpu_total']*1000):>10}   │ {results['python']/results['gpu_total']:>5.0f}x faster │\")\n",
    "    print(f\"│ GPU (kernel only)                   │ {format_time(results['kernel']*1000):>10}   │ {results['cpu']/results['kernel']:>5.0f}x faster │\")\n",
    "    print(f\"│ GPU ({n_ops} ops, keeps data)            │ {format_time(results['gpu_multi']*1000):>10}   │ {results['cpu_multi']/results['gpu_multi']:>5.1f}x faster │\")\n",
    "    print(\"└─────────────────────────────────────┴──────────────┴──────────────┘\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-code-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
