{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e876a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, cuda\n",
    "import time as time_module\n",
    "\n",
    "# 1. Pure Python version (baseline - slowest)\n",
    "def vector_add_python(a, b, c):\n",
    "    for i in range(len(a)):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# 2. Numba CPU version (JIT compiled)\n",
    "@jit(nopython=True)\n",
    "def vector_add_numba_cpu(a, b, c):\n",
    "    for i in range(len(a)):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "# 3. Numba CUDA version (GPU kernel)\n",
    "@cuda.jit\n",
    "def vector_add_cuda_kernel(a, b, c):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < c.size:\n",
    "        c[idx] = a[idx] + b[idx]\n",
    "\n",
    "def vector_add_cuda(a, b):\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    \n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    \n",
    "    c = d_c.copy_to_host()\n",
    "    return c\n",
    "\n",
    "def vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid):\n",
    "    \"\"\"GPU kernel without memory transfers - data already on GPU\"\"\"\n",
    "    vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    cuda.synchronize()\n",
    "\n",
    "def multiple_operations_gpu(a, b, n_ops=10):\n",
    "    \"\"\"Perform multiple additions - data stays on GPU\"\"\"\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    \n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    for _ in range(n_ops):\n",
    "        vector_add_cuda_kernel[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "    \n",
    "    cuda.synchronize()\n",
    "    return d_c.copy_to_host()\n",
    "\n",
    "def benchmark(func, *args, name=\"Function\", warmup=True):\n",
    "    if warmup:\n",
    "        func(*args)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time_module.perf_counter()\n",
    "        result = func(*args)\n",
    "        end = time_module.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    return avg_time, result\n",
    "\n",
    "def check_cuda():\n",
    "    try:\n",
    "        cuda.select_device(0)\n",
    "        cuda.current_context()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âš  CUDA not available: {e}\\n\")\n",
    "        return False\n",
    "\n",
    "def format_time(ms):\n",
    "    \"\"\"Format time with appropriate units\"\"\"\n",
    "    if ms >= 1000:\n",
    "        return f\"{ms/1000:.2f} s \"\n",
    "    elif ms >= 1:\n",
    "        return f\"{ms:.2f} ms\"\n",
    "    else:\n",
    "        return f\"{ms*1000:.2f} Î¼s\"\n",
    "\n",
    "def print_header(title, char=\"=\", width=80):\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(f\"{title:^{width}}\")\n",
    "    print(f\"{char * width}\")\n",
    "\n",
    "def print_section(title, width=80):\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * width)\n",
    "\n",
    "def print_result(label, time_ms, width=50):\n",
    "    time_str = format_time(time_ms)\n",
    "    print(f\"  {label:<{width-15}} {time_str:>12}\")\n",
    "\n",
    "def print_speedup(label, speedup, width=50):\n",
    "    if speedup >= 1:\n",
    "        print(f\"  {label:<{width-15}} {speedup:>10.1f}x faster\")\n",
    "    else:\n",
    "        print(f\"  {label:<{width-15}} {1/speedup:>10.1f}x slower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75cc99e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                           VECTOR ADDITION BENCHMARK                            \n",
      "================================================================================\n",
      "Array size: 10,000,000 elements (40.0 MB per array)\n",
      "\n",
      "1. Pure Python (baseline)\n",
      "--------------------------------------------------------------------------------\n",
      "  Execution time                         695.77 ms\n",
      "\n",
      "2. Numba CPU (JIT compiled)\n",
      "--------------------------------------------------------------------------------\n",
      "  Execution time                           3.06 ms\n",
      "  vs Pure Python                           227.0x faster\n",
      "\n",
      "3. Numba CUDA (with memory transfers)\n",
      "--------------------------------------------------------------------------------\n",
      "  Total time (with transfers)             11.80 ms\n",
      "  vs Pure Python                            59.0x faster\n",
      "  vs Numba CPU                               3.8x slower\n",
      "\n",
      "   GPU Time Breakdown\n",
      "--------------------------------------------------------------------------------\n",
      "  Transfer to GPU                          4.87 ms\n",
      "  Kernel execution                        79.75 Î¼s\n",
      "  Transfer from GPU                        6.01 ms\n",
      "--------------------------------------------------------------------------------\n",
      "  TOTAL                                   10.96 ms\n",
      "\n",
      "  ðŸ’¡ Memory transfer takes 136x longer than computation!\n",
      "\n",
      "4. Multiple Operations (data stays on GPU)\n",
      "--------------------------------------------------------------------------------\n",
      "  CPU (10 operations)                     28.48 ms\n",
      "  GPU (10 operations)                     12.51 ms\n",
      "  GPU vs CPU (10 ops)                        2.3x faster\n",
      "\n",
      "VERIFICATION\n",
      "--------------------------------------------------------------------------------\n",
      "  âœ“ CPU results match Python:  PASS\n",
      "  âœ“ GPU results match Python:  PASS\n",
      "\n",
      "================================================================================\n",
      "                              PERFORMANCE SUMMARY                               \n",
      "================================================================================\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ Implementation                      â”‚ Time         â”‚ Speedup      â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Pure Python (baseline)              â”‚  695.77 ms   â”‚ 1.0x         â”‚\n",
      "â”‚ Numba CPU                           â”‚    3.06 ms   â”‚   227x faster â”‚\n",
      "â”‚ GPU (with transfers)                â”‚   11.80 ms   â”‚    59x faster â”‚\n",
      "â”‚ GPU (kernel only)                   â”‚   79.75 Î¼s   â”‚    38x faster â”‚\n",
      "â”‚ GPU (10 ops, keeps data)            â”‚   12.51 ms   â”‚   2.3x faster â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "N = 10_000_000\n",
    "a = np.random.rand(N).astype(np.float32)\n",
    "b = np.random.rand(N).astype(np.float32)\n",
    "\n",
    "print_header(\"VECTOR ADDITION BENCHMARK\")\n",
    "print(f\"Array size: {N:,} elements ({N*4/1e6:.1f} MB per array)\")\n",
    "\n",
    "cuda_available = check_cuda()\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# ========================================================================\n",
    "# 1. Pure Python\n",
    "# ========================================================================\n",
    "print_section(\"1. Pure Python (baseline)\")\n",
    "c_python = np.zeros_like(a)\n",
    "time_python, _ = benchmark(vector_add_python, a, b, c_python, \n",
    "                            name=\"Pure Python\", warmup=False)\n",
    "results['python'] = time_python\n",
    "print_result(\"Execution time\", time_python * 1000)\n",
    "\n",
    "# ========================================================================\n",
    "# 2. Numba CPU\n",
    "# ========================================================================\n",
    "print_section(\"2. Numba CPU (JIT compiled)\")\n",
    "c_cpu = np.zeros_like(a)\n",
    "time_cpu, _ = benchmark(vector_add_numba_cpu, a, b, c_cpu, name=\"Numba CPU\")\n",
    "results['cpu'] = time_cpu\n",
    "print_result(\"Execution time\", time_cpu * 1000)\n",
    "print_speedup(\"vs Pure Python\", time_python / time_cpu)\n",
    "\n",
    "# ========================================================================\n",
    "# 3. GPU Benchmarks\n",
    "# ========================================================================\n",
    "if cuda_available:\n",
    "    print_section(\"3. Numba CUDA (with memory transfers)\")\n",
    "    time_cuda, c_cuda = benchmark(vector_add_cuda, a, b, name=\"GPU\")\n",
    "    results['gpu_total'] = time_cuda\n",
    "    print_result(\"Total time (with transfers)\", time_cuda * 1000)\n",
    "    print_speedup(\"vs Pure Python\", time_python / time_cuda)\n",
    "    print_speedup(\"vs Numba CPU\", time_cpu / time_cuda)\n",
    "    \n",
    "    # Break down GPU time\n",
    "    print_section(\"   GPU Time Breakdown\")\n",
    "    \n",
    "    # Transfer TO GPU\n",
    "    start = time_module.perf_counter()\n",
    "    d_a = cuda.to_device(a)\n",
    "    d_b = cuda.to_device(b)\n",
    "    transfer_to = time_module.perf_counter() - start\n",
    "    results['transfer_to'] = transfer_to\n",
    "    \n",
    "    # Kernel execution\n",
    "    d_c = cuda.device_array_like(a)\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (a.size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid)\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(5):\n",
    "        start = time_module.perf_counter()\n",
    "        vector_add_cuda_no_transfer(d_a, d_b, d_c, threads_per_block, blocks_per_grid)\n",
    "        times.append(time_module.perf_counter() - start)\n",
    "    kernel_time = np.mean(times)\n",
    "    results['kernel'] = kernel_time\n",
    "    \n",
    "    # Transfer FROM GPU\n",
    "    start = time_module.perf_counter()\n",
    "    result = d_c.copy_to_host()\n",
    "    transfer_from = time_module.perf_counter() - start\n",
    "    results['transfer_from'] = transfer_from\n",
    "    \n",
    "    total_breakdown = transfer_to + kernel_time + transfer_from\n",
    "    \n",
    "    print_result(\"Transfer to GPU\", transfer_to * 1000)\n",
    "    print_result(\"Kernel execution\", kernel_time * 1000)\n",
    "    print_result(\"Transfer from GPU\", transfer_from * 1000)\n",
    "    print(\"-\" * 80)\n",
    "    print_result(\"TOTAL\", total_breakdown * 1000)\n",
    "    \n",
    "    print(f\"\\n  ðŸ’¡ Memory transfer takes {(transfer_to + transfer_from)/kernel_time:.0f}x longer than computation!\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 4. Multiple Operations\n",
    "    # ====================================================================\n",
    "    print_section(\"4. Multiple Operations (data stays on GPU)\")\n",
    "    n_ops = 10\n",
    "    \n",
    "    # CPU\n",
    "    c_cpu = np.zeros_like(a)\n",
    "    start = time_module.perf_counter()\n",
    "    for _ in range(n_ops):\n",
    "        vector_add_numba_cpu(a, b, c_cpu)\n",
    "    time_cpu_multi = time_module.perf_counter() - start\n",
    "    results['cpu_multi'] = time_cpu_multi\n",
    "    \n",
    "    # GPU\n",
    "    time_gpu_multi, _ = benchmark(multiple_operations_gpu, a, b, n_ops, warmup=True)\n",
    "    results['gpu_multi'] = time_gpu_multi\n",
    "    \n",
    "    print_result(f\"CPU ({n_ops} operations)\", time_cpu_multi * 1000)\n",
    "    print_result(f\"GPU ({n_ops} operations)\", time_gpu_multi * 1000)\n",
    "    print_speedup(f\"GPU vs CPU ({n_ops} ops)\", time_cpu_multi / time_gpu_multi)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Verification\n",
    "    # ====================================================================\n",
    "    print_section(\"VERIFICATION\")\n",
    "    cpu_match = np.allclose(c_python, c_cpu)\n",
    "    gpu_match = np.allclose(c_python, c_cuda)\n",
    "    \n",
    "    print(f\"  âœ“ CPU results match Python:  {'PASS' if cpu_match else 'FAIL'}\")\n",
    "    print(f\"  âœ“ GPU results match Python:  {'PASS' if gpu_match else 'FAIL'}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Summary Table - Cleaner Format\n",
    "    # ====================================================================\n",
    "    print_header(\"PERFORMANCE SUMMARY\", \"=\")\n",
    "    \n",
    "    print(\"\\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "    print(\"â”‚ Implementation                      â”‚ Time         â”‚ Speedup      â”‚\")\n",
    "    print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
    "    print(f\"â”‚ Pure Python (baseline)              â”‚ {format_time(results['python']*1000):>10}   â”‚ 1.0x         â”‚\")\n",
    "    print(f\"â”‚ Numba CPU                           â”‚ {format_time(results['cpu']*1000):>10}   â”‚ {results['python']/results['cpu']:>5.0f}x faster â”‚\")\n",
    "    print(f\"â”‚ GPU (with transfers)                â”‚ {format_time(results['gpu_total']*1000):>10}   â”‚ {results['python']/results['gpu_total']:>5.0f}x faster â”‚\")\n",
    "    print(f\"â”‚ GPU (kernel only)                   â”‚ {format_time(results['kernel']*1000):>10}   â”‚ {results['cpu']/results['kernel']:>5.0f}x faster â”‚\")\n",
    "    print(f\"â”‚ GPU ({n_ops} ops, keeps data)            â”‚ {format_time(results['gpu_multi']*1000):>10}   â”‚ {results['cpu_multi']/results['gpu_multi']:>5.1f}x faster â”‚\")\n",
    "    print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-code-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
