{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d972ef8f",
   "metadata": {},
   "source": [
    "## Model and Optimizer State Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e154439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model State Dict ---\n",
      "fc1.weight           | torch.Size([5, 10]) \n",
      "fc1.bias             | torch.Size([5])     \n",
      "fc2.weight           | torch.Size([1, 5])  \n",
      "fc2.bias             | torch.Size([1])     \n",
      "\n",
      "RAW VIEW\n",
      "OrderedDict({'fc1.weight': tensor([[-0.1367, -0.2585,  0.1300, -0.2752, -0.3076, -0.0153,  0.0634, -0.0610,\n",
      "         -0.1233,  0.0261],\n",
      "        [-0.2843, -0.2743, -0.1469,  0.0816,  0.2899,  0.1792,  0.2866,  0.2788,\n",
      "         -0.0625, -0.1449],\n",
      "        [-0.1778,  0.1306, -0.0481, -0.0356,  0.1365, -0.1782, -0.0668, -0.0612,\n",
      "          0.1603,  0.2282],\n",
      "        [-0.2512,  0.2352,  0.0576,  0.2973,  0.1342, -0.2308,  0.1454, -0.2697,\n",
      "          0.3122,  0.0999],\n",
      "        [ 0.1420, -0.3055,  0.0207,  0.2566,  0.1396,  0.1423,  0.1613,  0.1129,\n",
      "          0.1024,  0.2644]]), 'fc1.bias': tensor([-0.1791, -0.0276,  0.2827, -0.1459,  0.1174]), 'fc2.weight': tensor([[-0.4019,  0.3250, -0.1255, -0.0877,  0.3886]]), 'fc2.bias': tensor([-0.1000])})\n",
      "\n",
      "--- Optimizer State Dict ---\n",
      "state                | {}\n",
      "param_groups         | [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': False, 'params': [0, 1, 2, 3]}]\n",
      "\n",
      "RAW VIEW\n",
      "{'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'decoupled_weight_decay': False, 'params': [0, 1, 2, 3]}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define a simple dummy model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SimpleModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- PRINTING STATE DICTS ---\n",
    "\n",
    "print(\"--- Model State Dict ---\")\n",
    "# The keys are the layer names, values are the tensors\n",
    "for param_tensor in model.state_dict():\n",
    "    print(f\"{param_tensor:<20} | {str(model.state_dict()[param_tensor].size()):<20}\")\n",
    "\n",
    "print(\"\\nRAW VIEW\")\n",
    "print(model.state_dict())\n",
    "\n",
    "print(\"\\n--- Optimizer State Dict ---\")\n",
    "# Optimizer state dict contains parameter groups and internal state (momentum, etc.)\n",
    "for var_name in optimizer.state_dict():\n",
    "    # We print just the keys or summary to avoid dumping massive tensors\n",
    "    print(f\"{var_name:<20} | {optimizer.state_dict()[var_name]}\")\n",
    "\n",
    "print(\"\\nRAW VIEW\")\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc5d5e",
   "metadata": {},
   "source": [
    "## Scheduler State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20149a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. StepLR State Dict ===\n",
      "Current LR: [0.010000000000000002]\n",
      "{'_get_lr_called_within_step': False,\n",
      " '_is_initial': False,\n",
      " '_last_lr': [0.010000000000000002],\n",
      " '_step_count': 8,\n",
      " 'base_lrs': [0.1],\n",
      " 'gamma': 0.1,\n",
      " 'last_epoch': 7,\n",
      " 'step_size': 5}\n",
      "\n",
      "========================================\n",
      "\n",
      "=== 2. CosineAnnealingLR State Dict ===\n",
      "Current LR: [0.0505]\n",
      "{'T_max': 50,\n",
      " '_get_lr_called_within_step': False,\n",
      " '_is_initial': False,\n",
      " '_last_lr': [0.0505],\n",
      " '_step_count': 26,\n",
      " 'base_lrs': [0.1],\n",
      " 'eta_min': 0.001,\n",
      " 'last_epoch': 25}\n",
      "\n",
      "========================================\n",
      "\n",
      "=== 3. ReduceLROnPlateau State Dict ===\n",
      "{'_last_lr': [0.05],\n",
      " 'best': 0.9,\n",
      " 'cooldown': 0,\n",
      " 'cooldown_counter': 0,\n",
      " 'default_min_lr': 0,\n",
      " 'eps': 1e-08,\n",
      " 'factor': 0.5,\n",
      " 'last_epoch': 6,\n",
      " 'min_lrs': [0],\n",
      " 'mode': 'min',\n",
      " 'mode_worse': inf,\n",
      " 'num_bad_epochs': 1,\n",
      " 'patience': 3,\n",
      " 'threshold': 0.0001,\n",
      " 'threshold_mode': 'rel'}\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import pprint\n",
    "\n",
    "# 1. Setup Dummy Model & Optimizer\n",
    "# (Schedulers need an optimizer to attach to)\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(\"=== 1. StepLR State Dict ===\")\n",
    "# Decays LR by 0.1 every 5 epochs\n",
    "scheduler_step = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Simulate 7 epochs (so we pass the step_size of 5)\n",
    "for i in range(7):\n",
    "    optimizer.step()\n",
    "    scheduler_step.step()\n",
    "\n",
    "print(f\"Current LR: {scheduler_step.get_last_lr()}\")\n",
    "pprint.pprint(scheduler_step.state_dict())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"=== 2. CosineAnnealingLR State Dict ===\")\n",
    "# Reset optimizer for clarity\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# Follows a cosine curve over 50 epochs\n",
    "scheduler_cos = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001)\n",
    "\n",
    "# Simulate 25 epochs (halfway through the cosine wave)\n",
    "for i in range(25):\n",
    "    optimizer.step()\n",
    "    scheduler_cos.step()\n",
    "\n",
    "print(f\"Current LR: {scheduler_cos.get_last_lr()}\")\n",
    "pprint.pprint(scheduler_cos.state_dict())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"=== 3. ReduceLROnPlateau State Dict ===\")\n",
    "# Reset optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# Reduce LR if validation loss stops going down for 3 epochs (patience=3)\n",
    "scheduler_plateau = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Simulate training where loss does NOT improve\n",
    "dummy_losses = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "\n",
    "for loss in dummy_losses:\n",
    "    optimizer.step()\n",
    "    # Note: This scheduler requires a metric argument in .step()\n",
    "    scheduler_plateau.step(loss)\n",
    "\n",
    "pprint.pprint(scheduler_plateau.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e8687",
   "metadata": {},
   "source": [
    "## RNG States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e9f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Initial Run ---\n",
      "Step 1: {'torch': 0.33669036626815796, 'numpy': 0.3745401188473625, 'python': 0.6394267984578837}\n",
      "Step 2: {'torch': 0.12880940735340118, 'numpy': 0.9507143064099162, 'python': 0.025010755222666936}\n",
      "--- 2. Continuing without stopping (Target behavior) ---\n",
      "Step 3: {'torch': 0.23446236550807953, 'numpy': 0.7319939418114051, 'python': 0.27502931836911926}\n",
      "Step 4: {'torch': 0.23033303022384644, 'numpy': 0.5986584841970366, 'python': 0.22321073814882275}\n",
      "\n",
      "... Crashing and Restarting ...\n",
      "\n",
      "--- 3. Resumed Run (Should match Target behavior) ---\n",
      "Step 3: {'torch': 0.23446236550807953, 'numpy': 0.7319939418114051, 'python': 0.27502931836911926}\n",
      "Step 4: {'torch': 0.23033303022384644, 'numpy': 0.5986584841970366, 'python': 0.22321073814882275}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 1. Setup: Define a function to generate \"random\" values from all libraries\n",
    "def get_random_values():\n",
    "    return {\n",
    "        \"torch\": torch.randn(1).item(),\n",
    "        \"numpy\": np.random.rand(),\n",
    "        \"python\": random.random()\n",
    "    }\n",
    "\n",
    "# Seed everything initially for demonstration\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"--- 1. Initial Run ---\")\n",
    "print(f\"Step 1: {get_random_values()}\")\n",
    "print(f\"Step 2: {get_random_values()}\")\n",
    "\n",
    "# --- SAVE THE STATE HERE (Simulating a checkpoint at Step 2) ---\n",
    "checkpoint = {\n",
    "    'torch_rng': torch.get_rng_state(),\n",
    "    'numpy_rng': np.random.get_state(),\n",
    "    'python_rng': random.getstate(),\n",
    "    # If using GPU, you must also save: torch.cuda.get_rng_state()\n",
    "    # 'cuda_rng': torch.cuda.get_rng_state() \n",
    "}\n",
    "\n",
    "# Continue generating (The \"Ground Truth\" timeline)\n",
    "print(\"--- 2. Continuing without stopping (Target behavior) ---\")\n",
    "print(f\"Step 3: {get_random_values()}\")\n",
    "print(f\"Step 4: {get_random_values()}\")\n",
    "\n",
    "\n",
    "# --- SIMULATE RESTART ---\n",
    "print(\"\\n... Crashing and Restarting ...\\n\")\n",
    "\n",
    "# Reset seeds to prove we aren't just getting lucky\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Restore States\n",
    "torch.set_rng_state(checkpoint['torch_rng'])\n",
    "np.random.set_state(checkpoint['numpy_rng'])\n",
    "random.setstate(checkpoint['python_rng'])\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.set_rng_state(loaded_state['cuda_rng'])\n",
    "\n",
    "print(\"--- 3. Resumed Run (Should match Target behavior) ---\")\n",
    "print(f\"Step 3: {get_random_values()}\")\n",
    "print(f\"Step 4: {get_random_values()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ab436",
   "metadata": {},
   "source": [
    "## Gradient Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53df62c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Scale: 65536.0\n",
      "Scale after step: 65536.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# 1. SETUP\n",
    "# Check for GPU (GradScaler needs a GPU to actually do work, though it runs no-ops on CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a simple dummy model\n",
    "model = nn.Linear(10, 1).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "# --- MAKE FAKE DATA ---\n",
    "# Batch Size: 32, Features: 10\n",
    "# We move it to the same device as the model\n",
    "inputs = torch.randn(32, 10).to(device)\n",
    "targets = torch.randn(32, 1).to(device)\n",
    "\n",
    "print(f\"Initial Scale: {scaler.get_scale()}\")\n",
    "\n",
    "# 2. TRAINING STEP\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# A. Forward pass in autocast context\n",
    "with torch.amp.autocast(str(device), enabled=torch.cuda.is_available()):\n",
    "    outputs = model(inputs)\n",
    "    loss = nn.MSELoss()(outputs, targets)\n",
    "\n",
    "# B. Scale the loss\n",
    "# This multiplies the loss by the scale factor (initially 65536)\n",
    "scaler.scale(loss).backward()\n",
    "\n",
    "# C. Step the optimizer\n",
    "scaler.step(optimizer)\n",
    "\n",
    "# D. Update the scale factor\n",
    "scaler.update()\n",
    "\n",
    "print(f\"Scale after step: {scaler.get_scale()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a297967",
   "metadata": {},
   "source": [
    "## Putting it all Together with MLFLOW\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bd5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hinson/anaconda3/envs/blog-code-examples/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Run ID: c7b613f1ca4b4798a26c17c35d953bed\n",
      "Starting Training on cuda...\n",
      "Epoch 1 | Loss: 0.8749\n",
      "Epoch 2 | Loss: 1.1350\n",
      "Epoch 3 | Loss: 0.9264\n",
      "Epoch 4 | Loss: 1.0717\n",
      "Epoch 5 | Loss: 0.7526\n",
      "\n",
      "Saving Checkpoint...\n",
      "{'config': {'device': 'cuda',\n",
      "            'epochs': 5,\n",
      "            'experiment_name': 'Gradient_Rage',\n",
      "            'hidden_dim': 32,\n",
      "            'input_dim': 10,\n",
      "            'lr': 0.01,\n",
      "            'output_dim': 1,\n",
      "            'run_name': 'Saving_Model_Checkpoint'},\n",
      " 'epoch': 5,\n",
      " 'model_state_dict': OrderedDict([('net.0.weight',\n",
      "                                   tensor([[ 0.1616, -0.2028, -0.0346, -0.2412, -0.1331,  0.0787,  0.1502,  0.2817,\n",
      "         -0.3122,  0.1913],\n",
      "        [-0.2026,  0.0965,  0.2875, -0.0150,  0.2968, -0.2882, -0.0999,  0.1871,\n",
      "         -0.0868, -0.0436],\n",
      "        [-0.1530,  0.2140,  0.1355,  0.0988, -0.1569, -0.2357,  0.0126,  0.0894,\n",
      "         -0.1623, -0.1090],\n",
      "        [ 0.1932,  0.0426,  0.0241, -0.0590, -0.2702, -0.2766, -0.1719, -0.1670,\n",
      "          0.0768, -0.1139],\n",
      "        [-0.1030,  0.2808,  0.1302,  0.2284, -0.1739, -0.1500,  0.1383, -0.2055,\n",
      "         -0.1849, -0.0116],\n",
      "        [-0.0953, -0.2274,  0.1663,  0.2343,  0.0303, -0.0767, -0.2947, -0.1365,\n",
      "          0.0881,  0.1684],\n",
      "        [-0.0194, -0.1268,  0.1725, -0.2488,  0.1789, -0.1440,  0.0242, -0.0660,\n",
      "          0.0193, -0.0769],\n",
      "        [ 0.2093, -0.0135,  0.2281,  0.3020, -0.1327, -0.2322,  0.2320,  0.2669,\n",
      "         -0.2615, -0.0418],\n",
      "        [-0.2918, -0.2391,  0.0524,  0.0625,  0.0461, -0.1076,  0.3131,  0.0494,\n",
      "         -0.1746, -0.0135],\n",
      "        [ 0.0964,  0.0897, -0.2384,  0.0988, -0.1833, -0.2842, -0.1520,  0.2849,\n",
      "          0.2816,  0.2778],\n",
      "        [ 0.1647,  0.2477, -0.0221, -0.1871,  0.1778, -0.1481, -0.1942, -0.1209,\n",
      "          0.0828,  0.1427],\n",
      "        [-0.1431, -0.1962,  0.0462, -0.1886, -0.1848,  0.1842, -0.0744,  0.3089,\n",
      "          0.3029,  0.0761],\n",
      "        [ 0.0919,  0.0757,  0.0121, -0.0430,  0.0198,  0.0994,  0.2866, -0.1023,\n",
      "         -0.1230, -0.0982],\n",
      "        [-0.1316,  0.0439,  0.2415,  0.2280,  0.1247, -0.2537, -0.1483, -0.0679,\n",
      "         -0.0466,  0.0385],\n",
      "        [-0.2697,  0.0819, -0.2570, -0.2219, -0.2826,  0.2565, -0.2776,  0.2195,\n",
      "         -0.0069, -0.0170],\n",
      "        [ 0.1764, -0.0375,  0.1971,  0.2258, -0.2536,  0.1408,  0.0379,  0.0886,\n",
      "          0.0340,  0.0928],\n",
      "        [-0.1727, -0.0203,  0.2791,  0.2100,  0.0086, -0.2785, -0.0075, -0.2202,\n",
      "          0.2510, -0.2141],\n",
      "        [-0.0429, -0.0914,  0.0600,  0.2955,  0.0506,  0.1814,  0.2314,  0.2750,\n",
      "         -0.1173, -0.0652],\n",
      "        [ 0.1313, -0.1042,  0.1542, -0.0794, -0.1957,  0.1486,  0.0273,  0.1356,\n",
      "          0.3087,  0.1966],\n",
      "        [ 0.2789,  0.0348, -0.2597, -0.2015, -0.1806, -0.2584, -0.1565, -0.2996,\n",
      "         -0.3133, -0.2109],\n",
      "        [-0.3322, -0.2151,  0.1005,  0.2856, -0.0724, -0.0704,  0.1475, -0.1367,\n",
      "         -0.1631,  0.1869],\n",
      "        [-0.2559,  0.0450,  0.2985,  0.0568,  0.0645, -0.2050,  0.0166,  0.0209,\n",
      "          0.0802,  0.1252],\n",
      "        [ 0.2951, -0.2620, -0.0016, -0.1783, -0.2679, -0.0470, -0.2592,  0.1990,\n",
      "          0.0329, -0.1746],\n",
      "        [ 0.1456, -0.3198, -0.1092, -0.0420, -0.2487, -0.1794, -0.0713,  0.2602,\n",
      "          0.2985,  0.2049],\n",
      "        [-0.1685, -0.1870,  0.2953,  0.1150, -0.0133,  0.0224, -0.2333, -0.1005,\n",
      "          0.3101, -0.0588],\n",
      "        [ 0.0048, -0.2730, -0.1876, -0.0029, -0.2145, -0.0596,  0.1493, -0.2873,\n",
      "          0.1067, -0.1879],\n",
      "        [-0.1596,  0.1424,  0.2541, -0.0650, -0.1088, -0.2651, -0.1603, -0.2883,\n",
      "         -0.0922,  0.0616],\n",
      "        [-0.3178,  0.2569,  0.1771, -0.3312, -0.1418,  0.1015, -0.0832, -0.0533,\n",
      "         -0.1542,  0.2749],\n",
      "        [-0.1968,  0.2771,  0.0262,  0.0592,  0.2672, -0.0331, -0.1785, -0.0535,\n",
      "          0.2543, -0.2310],\n",
      "        [ 0.0914, -0.2991,  0.1618,  0.0962, -0.0078,  0.1159,  0.0217,  0.1550,\n",
      "         -0.2333, -0.0490],\n",
      "        [ 0.1442,  0.2949,  0.1580, -0.2097,  0.2226, -0.0872, -0.0495, -0.2830,\n",
      "         -0.0555,  0.2264],\n",
      "        [ 0.0224, -0.1753,  0.1050, -0.0807,  0.3006, -0.2532, -0.0902, -0.3273,\n",
      "         -0.1440,  0.0803]], device='cuda:0')),\n",
      "                                  ('net.0.bias',\n",
      "                                   tensor([ 0.2887,  0.0333,  0.3044,  0.0430,  0.2004,  0.2318,  0.2441, -0.1941,\n",
      "         0.2712,  0.2073, -0.1410, -0.1528, -0.2718,  0.0866, -0.2770,  0.1244,\n",
      "         0.0974, -0.2764,  0.0852, -0.0442,  0.1636,  0.1829, -0.2980,  0.2845,\n",
      "         0.2670,  0.1329, -0.2814, -0.0913, -0.0715, -0.1024,  0.1839,  0.1309],\n",
      "       device='cuda:0')),\n",
      "                                  ('net.2.weight',\n",
      "                                   tensor([[ 0.1222, -0.0681,  0.0489,  0.0430, -0.1645, -0.1519, -0.0866,  0.1022,\n",
      "         -0.1451, -0.1637,  0.0809,  0.0935,  0.1316, -0.1348, -0.1108,  0.0747,\n",
      "          0.1340,  0.1251,  0.0128,  0.0406,  0.1894, -0.1178, -0.0972,  0.1070,\n",
      "         -0.1012, -0.1071,  0.1090,  0.0769,  0.0779, -0.0534,  0.0046,  0.1424]],\n",
      "       device='cuda:0')),\n",
      "                                  ('net.2.bias',\n",
      "                                   tensor([-0.1090], device='cuda:0'))]),\n",
      " 'optimizer_state_dict': {'param_groups': [{'amsgrad': False,\n",
      "                                            'betas': (0.9, 0.999),\n",
      "                                            'capturable': False,\n",
      "                                            'decoupled_weight_decay': False,\n",
      "                                            'differentiable': False,\n",
      "                                            'eps': 1e-08,\n",
      "                                            'foreach': None,\n",
      "                                            'fused': None,\n",
      "                                            'initial_lr': 0.01,\n",
      "                                            'lr': 0.0001,\n",
      "                                            'maximize': False,\n",
      "                                            'params': [0, 1, 2, 3],\n",
      "                                            'weight_decay': 0}],\n",
      "                          'state': {0: {'exp_avg': tensor([[ 2.4584e-03,  1.8629e-04, -2.3204e-03, -4.0654e-03, -9.5615e-03,\n",
      "         -4.9381e-03,  5.6667e-03, -5.1266e-03,  6.8712e-03,  2.3006e-03],\n",
      "        [-1.3817e-03,  1.7978e-03,  8.0487e-04,  2.9393e-03,  1.8026e-04,\n",
      "          1.6070e-04, -5.5815e-03,  8.1837e-03,  4.3057e-04, -2.9504e-03],\n",
      "        [ 1.2321e-03, -2.7141e-03,  1.1092e-03, -3.2572e-03, -1.5926e-03,\n",
      "         -2.2777e-03,  2.3596e-03, -3.6388e-03,  2.1811e-03,  2.9438e-03],\n",
      "        [ 2.4160e-03, -5.0726e-03, -7.1198e-04, -4.2913e-04, -1.9226e-03,\n",
      "         -1.4602e-03,  2.2451e-04, -1.4128e-03,  2.5708e-03,  4.1545e-03],\n",
      "        [-6.0535e-04,  4.8801e-03,  2.7087e-03,  1.1602e-02,  3.0843e-03,\n",
      "          3.6499e-03, -7.7580e-03,  7.6532e-03, -7.2420e-03, -2.8117e-03],\n",
      "        [-1.5671e-02, -8.0437e-04,  6.3190e-03,  1.1215e-02,  1.1091e-02,\n",
      "          2.1806e-02, -2.9286e-03,  1.5595e-02,  4.6708e-03,  1.2878e-03],\n",
      "        [-8.2046e-03, -4.4599e-03, -2.3897e-03,  7.0553e-04,  4.5820e-03,\n",
      "          1.1117e-02, -4.7307e-03, -4.7425e-03, -1.7171e-03, -4.7914e-04],\n",
      "        [ 3.3383e-03, -4.3386e-03,  7.8844e-04, -9.4979e-03, -6.7152e-03,\n",
      "         -6.9817e-03,  7.6712e-03, -9.3167e-03,  9.4698e-03,  3.6553e-03],\n",
      "        [-1.1878e-02, -8.5812e-04,  8.0819e-04,  1.1108e-02,  1.3482e-02,\n",
      "          1.8581e-02, -2.9693e-03,  5.4621e-03, -6.5434e-04, -3.0379e-03],\n",
      "        [-1.1558e-02,  1.3894e-02,  3.0540e-03,  9.6901e-03,  1.9817e-02,\n",
      "          1.4143e-02, -1.5700e-02,  2.1283e-02, -4.8963e-03,  2.4644e-03],\n",
      "        [ 2.2446e-03, -4.1961e-04, -1.6537e-03,  1.2168e-03, -2.9059e-03,\n",
      "          9.7644e-04,  2.3402e-03,  4.8817e-04,  1.3250e-03,  1.4067e-03],\n",
      "        [ 7.7409e-03,  1.8389e-03,  2.7921e-03, -4.8981e-03, -8.4752e-03,\n",
      "         -8.9380e-03,  5.4259e-03, -6.7984e-03, -2.9366e-03,  1.8062e-03],\n",
      "        [ 9.7829e-04,  2.2505e-03,  2.4375e-03, -1.1013e-04, -3.5575e-03,\n",
      "          3.4277e-03, -2.1196e-03,  8.9668e-03,  4.5601e-03,  3.2787e-03],\n",
      "        [-8.0570e-03,  4.0209e-04,  4.1025e-03,  1.0191e-02,  5.9334e-04,\n",
      "          5.5689e-03, -3.9365e-03,  1.0600e-02, -6.3515e-03,  3.3387e-03],\n",
      "        [-8.6425e-03,  1.4305e-03,  3.9322e-03,  2.6781e-03,  9.5146e-04,\n",
      "         -3.4726e-04, -3.0030e-03,  1.1484e-02,  5.8052e-03, -8.7002e-03],\n",
      "        [ 6.2436e-03,  1.4393e-04, -2.6375e-03, -6.5741e-03, -5.4598e-03,\n",
      "         -4.1994e-03,  2.9454e-03, -8.0047e-03,  1.6618e-03,  1.9342e-03],\n",
      "        [ 1.3994e-02, -6.0684e-04,  6.0164e-03, -6.6263e-03, -2.2359e-03,\n",
      "         -1.2042e-02,  4.9302e-03, -2.0470e-03, -1.1069e-03,  7.7301e-03],\n",
      "        [ 3.8577e-03,  7.0860e-03,  2.3823e-04, -7.7006e-03, -4.7913e-03,\n",
      "         -5.2583e-03,  3.2331e-04, -7.3713e-03,  2.3868e-03,  2.8013e-03],\n",
      "        [ 1.6017e-03,  3.1506e-04,  1.2993e-04, -5.2255e-04, -1.5130e-03,\n",
      "         -1.7219e-03,  1.1658e-03, -6.8131e-04, -2.1699e-04,  2.2752e-04],\n",
      "        [ 1.6776e-03, -4.5588e-03, -1.5570e-03, -6.9261e-04, -1.4847e-03,\n",
      "         -2.1507e-03,  6.8878e-05, -1.9371e-04,  1.8810e-03,  2.8113e-03],\n",
      "        [ 1.5666e-02,  9.4933e-03, -1.4505e-03, -1.3575e-02, -9.7251e-03,\n",
      "         -1.9282e-02,  5.5547e-03, -8.2768e-03, -8.5525e-04, -7.9573e-04],\n",
      "        [-9.1714e-03, -9.9263e-04,  5.0232e-03,  9.8974e-03,  7.7681e-03,\n",
      "          1.3455e-02, -1.1744e-02,  4.4342e-03, -2.0383e-03,  3.0190e-04],\n",
      "        [-3.6800e-03,  9.4481e-03,  2.1271e-03,  4.2053e-03,  2.1681e-03,\n",
      "          7.3450e-03, -3.7782e-03,  6.9028e-03, -3.1938e-03, -6.2703e-03],\n",
      "        [ 1.1061e-02, -1.4029e-03, -4.8734e-03, -5.7839e-03, -1.2752e-02,\n",
      "         -1.6807e-02,  7.2777e-03, -1.3871e-02, -4.8051e-04,  1.7049e-03],\n",
      "        [-9.8918e-03, -1.3125e-03, -2.4294e-03,  6.0394e-03,  8.7781e-03,\n",
      "          1.0690e-02, -4.8312e-03,  4.8080e-03,  2.2170e-03, -6.4447e-04],\n",
      "        [-6.4204e-03,  2.5694e-03, -6.7049e-04,  4.9241e-04,  7.3278e-03,\n",
      "          1.2378e-02,  2.6057e-03, -6.7368e-03, -6.9317e-04, -4.8010e-03],\n",
      "        [ 3.8811e-03, -4.2552e-03,  1.1623e-03, -2.6604e-03, -2.6349e-03,\n",
      "         -4.2750e-03,  3.6023e-03, -1.2234e-04,  4.7919e-03, -3.4038e-03],\n",
      "        [ 3.1601e-03,  2.4016e-03,  1.2567e-03,  1.7397e-03,  3.4947e-04,\n",
      "          5.6638e-04, -5.9401e-04,  2.4609e-03,  5.9838e-04, -4.4476e-04],\n",
      "        [ 4.5453e-03, -2.2273e-03,  4.3827e-04, -1.0810e-03, -6.9137e-04,\n",
      "         -1.7608e-03,  5.6038e-03, -6.3643e-03, -1.9401e-03,  3.2877e-03],\n",
      "        [-2.8316e-03, -8.2607e-04,  4.5608e-05,  4.7685e-03,  3.8452e-03,\n",
      "          5.2742e-03, -2.1306e-03,  5.6897e-03, -1.0297e-03, -3.6259e-04],\n",
      "        [ 1.5779e-04,  1.6667e-04,  5.7038e-05,  1.5144e-05,  9.6605e-05,\n",
      "          1.0652e-04, -6.3691e-05,  3.0301e-04,  9.1966e-05,  6.9402e-05],\n",
      "        [ 8.9454e-03, -1.8990e-03,  8.1671e-04, -2.1621e-03, -1.0017e-02,\n",
      "         -1.3001e-02,  1.2457e-03,  5.7351e-03,  5.2074e-03, -4.8074e-04]],\n",
      "       device='cuda:0'),\n",
      "                                        'exp_avg_sq': tensor([[8.1560e-06, 2.8583e-06, 3.4443e-06, 2.0539e-06, 5.8699e-06, 5.0877e-06,\n",
      "         6.1699e-06, 2.1158e-06, 2.2283e-06, 3.5057e-06],\n",
      "        [1.5705e-06, 5.2806e-07, 5.6903e-07, 7.0751e-07, 7.7282e-07, 5.9847e-07,\n",
      "         2.3872e-06, 6.8920e-06, 8.1786e-07, 8.2811e-07],\n",
      "        [3.5862e-07, 8.7003e-07, 3.6967e-07, 1.3086e-06, 4.9950e-07, 7.0083e-07,\n",
      "         1.1988e-06, 1.9165e-06, 3.8231e-07, 4.6727e-07],\n",
      "        [6.8590e-07, 1.6791e-06, 3.9638e-07, 2.8725e-07, 1.2437e-06, 5.0460e-07,\n",
      "         1.7417e-07, 4.1745e-07, 3.4518e-07, 8.5660e-07],\n",
      "        [2.6578e-06, 3.4033e-06, 2.7083e-06, 9.3952e-06, 7.5035e-06, 5.3887e-06,\n",
      "         9.1022e-06, 1.0399e-05, 4.2961e-06, 2.3530e-06],\n",
      "        [2.3373e-05, 4.0526e-06, 1.4875e-06, 5.9055e-06, 8.2139e-06, 3.7371e-05,\n",
      "         9.5558e-06, 1.5179e-05, 5.1146e-06, 8.9331e-06],\n",
      "        [2.6470e-06, 1.3641e-06, 1.9676e-06, 3.6325e-07, 9.5990e-07, 9.7906e-06,\n",
      "         4.3430e-06, 3.3233e-06, 1.1305e-06, 1.3419e-06],\n",
      "        [4.0108e-06, 3.5686e-06, 7.7144e-07, 5.0576e-06, 4.0874e-06, 2.7530e-06,\n",
      "         6.4392e-06, 5.1286e-06, 3.6428e-06, 1.1171e-06],\n",
      "        [1.6357e-05, 9.2811e-07, 4.2324e-06, 7.6837e-06, 1.2594e-05, 2.0434e-05,\n",
      "         2.9813e-06, 8.4135e-06, 2.2340e-06, 3.0367e-06],\n",
      "        [1.5563e-05, 1.3881e-05, 2.7390e-06, 7.7824e-06, 2.1358e-05, 1.4131e-05,\n",
      "         2.0699e-05, 3.3203e-05, 3.7073e-06, 8.1585e-06],\n",
      "        [1.8134e-06, 1.1804e-06, 7.5063e-07, 4.7049e-07, 3.1839e-07, 1.7450e-06,\n",
      "         4.0066e-06, 1.1178e-06, 1.2256e-06, 2.1058e-06],\n",
      "        [6.7533e-06, 1.0757e-06, 1.9517e-06, 2.1181e-06, 5.3723e-06, 8.1317e-06,\n",
      "         3.4838e-06, 6.4191e-06, 2.2207e-06, 5.3708e-07],\n",
      "        [4.3497e-07, 3.8505e-06, 9.1482e-07, 2.6139e-07, 5.5212e-06, 1.2940e-06,\n",
      "         2.1385e-06, 4.9861e-06, 1.1531e-06, 4.7627e-07],\n",
      "        [9.1879e-06, 1.9097e-06, 1.6806e-06, 4.1218e-06, 2.4515e-06, 4.2886e-06,\n",
      "         4.7916e-06, 9.8890e-06, 9.1909e-06, 3.7114e-06],\n",
      "        [7.9392e-06, 1.1917e-06, 9.3890e-07, 4.4929e-07, 3.8507e-06, 3.4363e-06,\n",
      "         2.1768e-06, 1.2005e-05, 2.6211e-06, 3.9190e-06],\n",
      "        [3.5555e-06, 1.2933e-06, 6.2350e-07, 3.1644e-06, 3.3394e-06, 2.2495e-06,\n",
      "         1.9847e-06, 2.4338e-06, 8.5918e-07, 3.5873e-07],\n",
      "        [1.0315e-05, 4.4203e-07, 2.9777e-06, 6.2581e-06, 4.4976e-06, 1.2041e-05,\n",
      "         3.8657e-06, 7.9822e-06, 3.5429e-06, 1.9305e-06],\n",
      "        [4.9032e-06, 2.9952e-06, 1.0858e-06, 2.5757e-06, 3.2959e-06, 1.1267e-06,\n",
      "         1.0221e-06, 4.9481e-06, 9.1047e-07, 5.4852e-07],\n",
      "        [2.0053e-07, 5.0305e-08, 2.6128e-08, 3.7441e-08, 3.0628e-07, 2.3253e-07,\n",
      "         1.4618e-07, 4.8912e-08, 4.1159e-08, 1.5175e-08],\n",
      "        [5.0240e-07, 1.3560e-06, 8.3635e-07, 2.8754e-07, 4.1893e-07, 6.4777e-07,\n",
      "         2.0646e-07, 3.8739e-07, 1.3307e-07, 5.1364e-07],\n",
      "        [2.2943e-05, 5.0237e-06, 6.2423e-06, 6.2612e-06, 8.3828e-06, 2.7313e-05,\n",
      "         1.3902e-05, 9.3644e-06, 5.7595e-06, 1.1117e-05],\n",
      "        [9.5142e-06, 9.1243e-07, 2.0607e-06, 5.6979e-06, 5.6297e-06, 1.6238e-05,\n",
      "         1.0057e-05, 6.8442e-06, 4.3878e-06, 3.9324e-06],\n",
      "        [1.2610e-06, 5.1351e-06, 1.3658e-06, 1.6907e-06, 2.5099e-06, 2.7003e-06,\n",
      "         2.4595e-06, 1.9529e-06, 6.4718e-07, 2.1289e-06],\n",
      "        [1.1100e-05, 2.7463e-06, 2.9704e-06, 2.6823e-06, 1.0824e-05, 1.4124e-05,\n",
      "         6.0042e-06, 1.0482e-05, 9.9113e-07, 3.3675e-06],\n",
      "        [9.8064e-06, 3.9871e-07, 1.7176e-06, 3.4187e-06, 4.4934e-06, 1.3047e-05,\n",
      "         4.3425e-06, 4.5590e-06, 2.5441e-06, 2.9582e-06],\n",
      "        [2.8146e-06, 1.3750e-06, 1.3183e-06, 1.8192e-06, 4.8591e-06, 9.3001e-06,\n",
      "         1.4459e-06, 5.8868e-06, 9.0393e-07, 2.2498e-06],\n",
      "        [1.3567e-06, 1.7327e-06, 3.6908e-07, 1.1060e-06, 2.2438e-06, 3.0643e-06,\n",
      "         4.0609e-06, 7.4962e-07, 1.1323e-06, 3.6718e-06],\n",
      "        [1.1483e-06, 1.1885e-06, 2.5210e-07, 4.8398e-07, 8.5547e-07, 8.9758e-07,\n",
      "         7.6764e-07, 4.3445e-07, 6.5011e-07, 1.1318e-06],\n",
      "        [2.3450e-06, 9.7075e-07, 1.7863e-06, 2.1569e-07, 1.8322e-07, 2.1703e-06,\n",
      "         3.6236e-06, 4.2239e-06, 1.5223e-06, 1.1097e-06],\n",
      "        [2.2142e-06, 2.3797e-07, 5.3280e-07, 1.3594e-06, 1.2747e-06, 2.3898e-06,\n",
      "         1.0121e-06, 1.5440e-06, 2.0478e-07, 1.9639e-07],\n",
      "        [6.4977e-09, 1.8176e-09, 2.5716e-09, 3.0772e-10, 2.0599e-09, 2.2166e-09,\n",
      "         5.1385e-09, 6.6583e-09, 8.7366e-10, 4.0998e-09],\n",
      "        [2.9068e-06, 3.8670e-06, 4.6805e-06, 1.0868e-06, 5.7933e-06, 1.4510e-05,\n",
      "         9.3740e-06, 2.9225e-06, 1.6563e-06, 4.1625e-06]], device='cuda:0'),\n",
      "                                        'step': tensor(5.)},\n",
      "                                    1: {'exp_avg': tensor([-2.6818e-03,  4.2900e-03, -2.5826e-03,  1.0408e-03,  6.3125e-03,\n",
      "         1.3574e-02,  4.7647e-03, -3.0976e-03,  1.4139e-02,  1.2446e-03,\n",
      "        -2.5691e-03, -5.7214e-03, -3.0363e-03,  4.0943e-03,  6.1508e-03,\n",
      "        -1.4015e-03, -8.7278e-03, -8.8587e-03, -4.7422e-04, -1.2319e-04,\n",
      "        -1.3722e-02,  4.3811e-03, -5.3885e-04, -7.7991e-03,  1.1236e-02,\n",
      "         4.0172e-03, -1.3988e-03, -3.4865e-03, -4.5840e-03,  4.0177e-03,\n",
      "        -2.0314e-05, -2.8454e-03], device='cuda:0'),\n",
      "                                        'exp_avg_sq': tensor([7.4272e-06, 1.3134e-06, 1.3510e-06, 9.2872e-07, 1.7213e-05, 1.4115e-05,\n",
      "        3.1581e-06, 9.5437e-07, 1.0980e-05, 1.0383e-05, 1.8466e-06, 2.7332e-06,\n",
      "        7.2955e-07, 8.1862e-06, 3.3951e-06, 5.4098e-07, 3.4102e-06, 3.6481e-06,\n",
      "        4.5193e-08, 5.0788e-07, 1.3256e-05, 3.2193e-06, 7.1163e-07, 6.7953e-06,\n",
      "        8.3910e-06, 2.5874e-06, 2.6429e-06, 2.2443e-06, 3.2708e-06, 5.7752e-07,\n",
      "        5.9220e-09, 5.7850e-06], device='cuda:0'),\n",
      "                                        'step': tensor(5.)},\n",
      "                                    2: {'exp_avg': tensor([[-0.0026, -0.0446, -0.0372,  0.0228, -0.0239, -0.0482, -0.0119, -0.0178,\n",
      "         -0.0475, -0.0103, -0.0032, -0.0276, -0.0055, -0.0301, -0.0227, -0.0071,\n",
      "         -0.0053, -0.0319,  0.0066,  0.0101, -0.0461, -0.0269,  0.0158,  0.0293,\n",
      "         -0.0646, -0.0100,  0.0006, -0.0090, -0.0429, -0.0330, -0.0172, -0.0127]],\n",
      "       device='cuda:0'),\n",
      "                                        'exp_avg_sq': tensor([[1.5075e-04, 1.9301e-04, 1.9496e-04, 1.3358e-04, 9.3668e-05, 2.8581e-04,\n",
      "         4.9251e-05, 6.8619e-05, 9.3040e-05, 2.6017e-04, 5.7339e-05, 1.4336e-04,\n",
      "         6.8831e-06, 8.9203e-05, 1.8017e-04, 9.9347e-05, 2.3656e-05, 4.8163e-05,\n",
      "         9.5298e-05, 1.7960e-04, 1.0928e-04, 9.3154e-05, 5.1118e-05, 2.9414e-04,\n",
      "         3.1576e-04, 1.0033e-04, 1.4286e-05, 8.3163e-05, 8.7004e-05, 4.5216e-05,\n",
      "         1.6486e-04, 1.7992e-04]], device='cuda:0'),\n",
      "                                        'step': tensor(5.)},\n",
      "                                    3: {'exp_avg': tensor([-0.0812], device='cuda:0'),\n",
      "                                        'exp_avg_sq': tensor([0.0008], device='cuda:0'),\n",
      "                                        'step': tensor(5.)}}},\n",
      " 'rng_state': tensor([42,  0,  0,  ...,  0,  0,  0], dtype=torch.uint8),\n",
      " 'run_id': 'c7b613f1ca4b4798a26c17c35d953bed',\n",
      " 'scaler_state_dict': {'_growth_tracker': 5,\n",
      "                       'backoff_factor': 0.5,\n",
      "                       'growth_factor': 2.0,\n",
      "                       'growth_interval': 2000,\n",
      "                       'scale': 65536.0},\n",
      " 'scheduler_state_dict': {'_get_lr_called_within_step': False,\n",
      "                          '_is_initial': False,\n",
      "                          '_last_lr': [0.0001],\n",
      "                          '_step_count': 6,\n",
      "                          'base_lrs': [0.01],\n",
      "                          'gamma': 0.1,\n",
      "                          'last_epoch': 5,\n",
      "                          'step_size': 2}}\n",
      "Run c7b613f1ca4b4798a26c17c35d953bed Complete. Checkpoint saved to MLflow.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler\n",
    "import mlflow\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "RUN_ID = None\n",
    "# --- 1. CONFIGURATION ---\n",
    "config = {\n",
    "    \"experiment_name\": \"Gradient_Rage\",\n",
    "    \"run_name\": \"Saving_Model_Checkpoint\",\n",
    "    \"input_dim\": 10,\n",
    "    \"hidden_dim\": 32,\n",
    "    \"output_dim\": 1,\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "# --- 2. MODEL DEFINITION ---\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- 3. TRAINING FUNCTION ---\n",
    "def train():\n",
    "    global RUN_ID\n",
    "    # A. Setup MLflow\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "    mlflow.set_experiment(config[\"experiment_name\"])\n",
    "\n",
    "    # B. Start the Run -> Capture 'run' object\n",
    "    with mlflow.start_run(run_name=config[\"run_name\"]) as run:\n",
    "        \n",
    "        # --- CAPTURE RUN ID ---\n",
    "        RUN_ID = run.info.run_id\n",
    "        print(f\"Active Run ID: {RUN_ID}\")\n",
    "        \n",
    "        # 1. Log the Model Config\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        # 2. Initialize Components\n",
    "        model = SimpleModel(config['input_dim'], config['hidden_dim'], config['output_dim']).to(config['device'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "        scaler = GradScaler(enabled=(config['device'] == 'cuda'))\n",
    "\n",
    "        print(f\"Starting Training on {config['device']}...\")\n",
    "        \n",
    "        # 3. Training Loop\n",
    "        for epoch in range(config['epochs']):\n",
    "            model.train()\n",
    "            \n",
    "            inputs = torch.randn(32, config['input_dim']).to(config['device'])\n",
    "            targets = torch.randn(32, config['output_dim']).to(config['device'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(config['device'], enabled=(config['device'] == 'cuda')):\n",
    "                outputs = model(inputs)\n",
    "                loss = nn.MSELoss()(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # 4. Log Metrics\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "            mlflow.log_metric(\"learning_rate\", current_lr, step=epoch)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- 4. THE SAVE ---\n",
    "        print(\"\\nSaving Checkpoint...\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'run_id': RUN_ID,  # <--- SAVE ID HERE for traceability\n",
    "            'epoch': config['epochs'],\n",
    "            'config': config, \n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'rng_state': torch.get_rng_state(),\n",
    "        }\n",
    "        pprint.pprint(checkpoint)\n",
    "\n",
    "        local_path = \"checkpoint.pth\"\n",
    "        torch.save(checkpoint, local_path)\n",
    "        mlflow.log_artifact(local_path)\n",
    "        os.remove(local_path)\n",
    "        \n",
    "        print(f\"Run {RUN_ID} Complete. Checkpoint saved to MLflow.\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943862b",
   "metadata": {},
   "source": [
    "## Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5979487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [STEP B] Resuming from MLflow ID: c7b613f1ca4b4798a26c17c35d953bed ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73774074b15041ff870358e1a52d82d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [INSPECTION] Verifying Loaded Components ---\n",
      "1. Config Loaded: {'experiment_name': 'Gradient_Rage', 'run_name': 'Saving_Model_Checkpoint', 'input_dim': 10, 'hidden_dim': 32, 'output_dim': 1, 'lr': 0.01, 'epochs': 5, 'device': 'cuda'}\n",
      "2. Model Weights (First 3 of Layer 1): [0.1615852415561676, -0.20281507074832916, -0.03463708609342575]\n",
      "3. Optimizer LR (should be decayed): 0.0001\n",
      "4. Scheduler Last Epoch: 5\n",
      "5. Scaler Scale Factor: 65536.0\n",
      "6. RNG Test (Next Random Num): -2.0489625930786133\n"
     ]
    }
   ],
   "source": [
    "def resume_from_mlflow(run_id):\n",
    "    print(f\"\\n--- [STEP B] Resuming from MLflow ID: {run_id} ---\")\n",
    "    \n",
    "    # 1. Download\n",
    "    mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "    local_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"checkpoint.pth\")\n",
    "    \n",
    "    # 2. Load File\n",
    "    checkpoint = torch.load(local_path)\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # 3. Re-Init Architecture\n",
    "    model = SimpleModel(config['input_dim'], config['hidden_dim'], config['output_dim'])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "    scaler = GradScaler(enabled=(config['device'] == \"cuda\"))\n",
    "    \n",
    "    # 4. Load States\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    torch.set_rng_state(checkpoint['rng_state'])\n",
    "    \n",
    "    return model, optimizer, scheduler, scaler, config\n",
    "\n",
    "model, opt, sched, scaler, config = resume_from_mlflow(run_id=RUN_ID)\n",
    "    \n",
    "print(\"\\n--- [INSPECTION] Verifying Loaded Components ---\")\n",
    "\n",
    "# 1. CONFIG\n",
    "print(f\"1. Config Loaded: {config}\")\n",
    "\n",
    "# 2. MODEL WEIGHTS (Check first layer weights)\n",
    "print(f\"2. Model Weights (First 3 of Layer 1): {model.net[0].weight.view(-1)[:3].tolist()}\")\n",
    "\n",
    "# 3. OPTIMIZER (Check Param Groups)\n",
    "print(f\"3. Optimizer LR (should be decayed): {opt.param_groups[0]['lr']}\")\n",
    "\n",
    "# 4. SCHEDULER (Check Epoch Counter)\n",
    "print(f\"4. Scheduler Last Epoch: {sched.last_epoch}\")\n",
    "\n",
    "# 5. SCALER (Check Scale Factor)\n",
    "print(f\"5. Scaler Scale Factor: {scaler.get_scale()}\")\n",
    "\n",
    "# 6. RNG (Check a random number generation)\n",
    "print(f\"6. RNG Test (Next Random Num): {torch.randn(1).item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-code-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
